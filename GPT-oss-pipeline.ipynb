{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1caa41-2749-4bba-a2c1-7494afb1e856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# GPT-OSS forecasting template (no quantization, loads model once)\n",
    "# ======================================================================\n",
    "\n",
    "# --- Imports ---\n",
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# --- Global results dict + logger ---\n",
    "RESULTS = {}\n",
    "\n",
    "def log_simple_result(results_dict, dataset_name, horizon, mae, rmse, mape, r2):\n",
    "    results_dict[dataset_name] = {\n",
    "        \"horizon\": horizon,\n",
    "        \"MAE\": round(mae, 2),\n",
    "        \"RMSE\": round(rmse, 2),\n",
    "        \"MAPE (%)\": round(mape, 2),\n",
    "        \"R²\": round(r2, 4),\n",
    "    }\n",
    "\n",
    "# --- Load GPT-OSS once ---\n",
    "MODEL_ID = \"openai/gpt-oss-20b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "\n",
    "# --- Forecasting function ---\n",
    "def gptoss_prediction(\n",
    "    dataset,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    horizon=24,\n",
    "    frequency=\"M\",\n",
    "    dataset_name=None,\n",
    "    results_dict=RESULTS,\n",
    "    do_sample=False,\n",
    "    seed=42,\n",
    "):\n",
    "    \"\"\"\n",
    "    dataset: callable -> (train_df, test_df) with columns ['unique_id','ds','y']\n",
    "    Returns RESULTS dict and shows a plot.\n",
    "    \"\"\"\n",
    "    # 1) Load data\n",
    "    train_df, test_df = dataset(horizon=horizon)\n",
    "    y_train = train_df[\"y\"].values.astype(float)\n",
    "    y_test = test_df[\"y\"].values.astype(float)\n",
    "\n",
    "    # 2) Build prompt from raw training values\n",
    "    prompt = \" \".join(map(str, y_train.tolist()))\n",
    "\n",
    "    # 3) Generate continuation\n",
    "    torch.manual_seed(seed)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    gen = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=horizon * 4,   # allow more tokens, we'll parse tail\n",
    "        do_sample=do_sample,\n",
    "        temperature=1.0 if do_sample else None,\n",
    "        top_p=0.95 if do_sample else None,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    decoded = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
    "\n",
    "    # 4) Parse the LAST `horizon` numbers from the output\n",
    "    all_nums = [float(x) for x in re.findall(r\"\\d+\\.?\\d*\", decoded)]\n",
    "    needed_total = len(y_train) + horizon\n",
    "    if len(all_nums) < needed_total:\n",
    "        # pad if short\n",
    "        all_nums += [all_nums[-1] if all_nums else 0.0] * (needed_total - len(all_nums))\n",
    "    y_pred = np.array(all_nums[-horizon:], dtype=float)\n",
    "\n",
    "    # 5) Metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        ape = np.abs((y_test - y_pred) / y_test) * 100.0\n",
    "        mape = np.nanmean(ape)\n",
    "        if np.isnan(mape):\n",
    "            mape = float(\"inf\")\n",
    "    ss_res = np.sum((y_test - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_test - np.mean(y_test)) ** 2)\n",
    "    r2 = 1 - ss_res / ss_tot if ss_tot != 0 else float(\"nan\")\n",
    "\n",
    "    # 6) Log results\n",
    "    name = dataset_name or getattr(dataset, \"__name__\", \"unnamed_dataset\")\n",
    "    log_simple_result(results_dict, name, horizon, mae, rmse, mape, r2)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Colorblind-friendly palette\n",
    "    history_color = '#1b9e77'   # teal\n",
    "    observed_color = '#d95f02'  # muted orange\n",
    "    forecast_color = '#7570b3'  # muted purple\n",
    "\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    ax.plot(train_df['ds'], y_train, label=\"Historical Data\", color=history_color, linewidth=2)\n",
    "    ax.plot(test_df['ds'], y_test, label=\"Observed Future\", color=observed_color, markersize=6, linewidth=2)\n",
    "    ax.plot(test_df['ds'], y_pred, label=\"Model Forecast\", color=forecast_color, markersize=6, linewidth=2, linestyle='--')\n",
    "\n",
    "\n",
    "    ax.set_title(f\"Forecasting Monthly Air Passenger Counts Using GPT-OSS (H = {horizon})\",\n",
    "                fontsize=14, fontweight='bold', pad=15)\n",
    "    ax.set_xlabel(\"Date\", fontsize=12)\n",
    "    ax.set_ylabel(\"Number of Passengers\", fontsize=12)\n",
    "\n",
    "    ax.legend(fontsize=11, frameon=True, loc='upper left')\n",
    "    ax.tick_params(axis='x', rotation=30)\n",
    "    ax.margins(x=0.02)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('GPToss', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71310520-cb47-40bd-8bb0-bcd086cc9f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dataset loader (AirPassengers) ---\n",
    "def load_air_passengers(horizon=24, unique_id=\"AP1\"):\n",
    "    url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv\"\n",
    "    df = pd.read_csv(url)\n",
    "\n",
    "    df[\"Month\"] = pd.to_datetime(df[\"Month\"])\n",
    "    df = df.rename(columns={\"Month\": \"ds\", \"Passengers\": \"y\"})\n",
    "    print(df.shape)\n",
    "    print(\"Mean: \", df[\"y\"].mean())\n",
    "\n",
    "    train_df = df.iloc[:-horizon].copy()\n",
    "    test_df = df.iloc[-horizon:].copy()\n",
    "\n",
    "    train_df[\"unique_id\"] = unique_id\n",
    "    test_df[\"unique_id\"] = unique_id\n",
    "\n",
    "    return train_df[[\"unique_id\", \"ds\", \"y\"]], test_df[[\"unique_id\", \"ds\", \"y\"]]\n",
    "\n",
    "# --- Example run ---\n",
    "gptoss_prediction(load_air_passengers, model, tokenizer, horizon=24, frequency=\"M\")\n",
    "print(RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fd368c-d83a-4386-87df-ef293f4dc8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_air_sunsopts(horizon=24, unique_id=\"AP1\"):\n",
    "\n",
    "    url = \"https://raw.githubusercontent.com/ahamed14051/time-series-dataset/refs/heads/main/Sunspots.csv\"\n",
    "    df = pd.read_csv(url)\n",
    "    df = df.drop(columns=['Unnamed: 0'])\n",
    "    print(df.shape)\n",
    "\n",
    "    # Standardize column names\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df.rename(columns={\"Date\": \"ds\", \"Monthly Mean Total Sunspot Number\": \"y\"})\n",
    "    print('Mean: ', df['y'].mean())\n",
    "\n",
    "    # Split train/test\n",
    "    train_df = df.iloc[:-horizon].copy()\n",
    "    test_df = df.iloc[-horizon:].copy()\n",
    "\n",
    "    # Add unique_id\n",
    "    train_df['unique_id'] = unique_id\n",
    "    test_df['unique_id'] = unique_id\n",
    "\n",
    "    return train_df[['unique_id', 'ds', 'y']], test_df[['unique_id', 'ds', 'y']]\n",
    "\n",
    "gptoss_prediction(load_air_sunsopts, model, tokenizer, horizon=120, frequency=\"M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edf3cf0-a331-4285-b75f-46fdae2c3ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_temp(horizon=24, unique_id=\"AP1\"):\n",
    "    df = pd.read_csv(\"https://raw.githubusercontent.com/ahamed14051/time-series-dataset/refs/heads/main/temp.csv\")\n",
    "\n",
    "    # Remove unnamed columns\n",
    "    if 'Unnamed: 2' in df.columns:\n",
    "        df = df.drop(columns=['Unnamed: 2'])\n",
    "\n",
    "    # Parse date safely\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "\n",
    "    # Drop rows with invalid dates (like the description text)\n",
    "    df = df.dropna(subset=['Date'])\n",
    "\n",
    "    # Rename columns\n",
    "    df = df.rename(columns={\"Date\": \"ds\", \"temp\": \"y\"})\n",
    "\n",
    "    # Convert 'y' to numeric, coercing errors to NaN and dropping them\n",
    "    df['y'] = pd.to_numeric(df['y'], errors='coerce')\n",
    "    df = df.dropna(subset=['y'])\n",
    "\n",
    "    print('Mean:', df['y'].mean())\n",
    "\n",
    "    # Train/test split\n",
    "    train_df = df.iloc[:-horizon].copy()\n",
    "    test_df = df.iloc[-horizon:].copy()\n",
    "\n",
    "    # Add unique_id\n",
    "    train_df['unique_id'] = unique_id\n",
    "    test_df['unique_id'] = unique_id\n",
    "\n",
    "    return train_df[['unique_id', 'ds', 'y']], test_df[['unique_id', 'ds', 'y']]\n",
    "\n",
    "gptoss_prediction(load_temp, model, tokenizer, horizon=240, frequency=\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5ad053-19be-4253-946d-812f9dc86ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da83fafc-41a0-4857-8862-9baef0e6f9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_temperature(horizon=24, unique_id=\"AP1\"):\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(\"https://raw.githubusercontent.com/ahamed14051/time-series-dataset/refs/heads/main/Rainfall_data.csv\")\n",
    "\n",
    "    # Create timestamp from Year, Month, Day\n",
    "    df['ds'] = pd.to_datetime(df[['Year', 'Month', 'Day']])\n",
    "\n",
    "    # Use Temperature column as 'y'\n",
    "    df['y'] = pd.to_numeric(df['Temperature'], errors='coerce')\n",
    "    df = df.dropna(subset=['y'])\n",
    "\n",
    "    print('Mean Temperature:', df['y'].mean())\n",
    "\n",
    "    # Train/test split\n",
    "    train_df = df.iloc[:-horizon].copy()\n",
    "    test_df = df.iloc[-horizon:].copy()\n",
    "\n",
    "    # Add unique_id\n",
    "    train_df['unique_id'] = unique_id\n",
    "    test_df['unique_id'] = unique_id\n",
    "\n",
    "    return train_df[['unique_id', 'ds', 'y']], test_df[['unique_id', 'ds', 'y']]\n",
    "\n",
    "gptoss_prediction(load_temperature, model, tokenizer, horizon=24, frequency=\"M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e781c6-6ab5-4c8d-9202-ef20489addca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_precipitation(horizon=24, unique_id=\"AP1\"):\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(\"https://raw.githubusercontent.com/ahamed14051/time-series-dataset/refs/heads/main/Rainfall_data.csv\")\n",
    "\n",
    "    # Create timestamp from Year, Month, Day\n",
    "    df['ds'] = pd.to_datetime(df[['Year', 'Month', 'Day']])\n",
    "\n",
    "    # Use Temperature column as 'y'\n",
    "    df['y'] = pd.to_numeric(df['Precipitation'], errors='coerce')\n",
    "    df = df.dropna(subset=['y'])\n",
    "\n",
    "    print('Mean Precipitation:', df['y'].mean())\n",
    "\n",
    "    # Train/test split\n",
    "    train_df = df.iloc[:-horizon].copy()\n",
    "    test_df = df.iloc[-horizon:].copy()\n",
    "\n",
    "    # Add unique_id\n",
    "    train_df['unique_id'] = unique_id\n",
    "    test_df['unique_id'] = unique_id\n",
    "\n",
    "    return train_df[['unique_id', 'ds', 'y']], test_df[['unique_id', 'ds', 'y']]\n",
    "\n",
    "gptoss_prediction(load_precipitation, model, tokenizer, horizon=24, frequency=\"M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fe1aad-493f-48f2-b87e-002c7a054b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_humidity(horizon=24, unique_id=\"AP1\"):\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(\"https://raw.githubusercontent.com/ahamed14051/time-series-dataset/refs/heads/main/Rainfall_data.csv\")\n",
    "\n",
    "    # Create timestamp from Year, Month, Day\n",
    "    df['ds'] = pd.to_datetime(df[['Year', 'Month', 'Day']])\n",
    "\n",
    "    # Use Temperature column as 'y'\n",
    "    df['y'] = pd.to_numeric(df['Specific Humidity'], errors='coerce')\n",
    "    df = df.dropna(subset=['y'])\n",
    "\n",
    "    print('Mean Specific Humidity:', df['y'].mean())\n",
    "\n",
    "    # Train/test split\n",
    "    train_df = df.iloc[:-horizon].copy()\n",
    "    test_df = df.iloc[-horizon:].copy()\n",
    "\n",
    "    # Add unique_id\n",
    "    train_df['unique_id'] = unique_id\n",
    "    test_df['unique_id'] = unique_id\n",
    "\n",
    "    return train_df[['unique_id', 'ds', 'y']], test_df[['unique_id', 'ds', 'y']]\n",
    "\n",
    "gptoss_prediction(load_humidity, model, tokenizer, horizon=24, frequency=\"M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be686400-cde3-4fce-bd51-3057d7034bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_relative_humidity(horizon=24, unique_id=\"AP1\"):\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(\"https://raw.githubusercontent.com/ahamed14051/time-series-dataset/refs/heads/main/Rainfall_data.csv\")\n",
    "\n",
    "    # Create timestamp from Year, Month, Day\n",
    "    df['ds'] = pd.to_datetime(df[['Year', 'Month', 'Day']])\n",
    "\n",
    "    # Use Temperature column as 'y'\n",
    "    df['y'] = pd.to_numeric(df['Relative Humidity'], errors='coerce')\n",
    "    df = df.dropna(subset=['y'])\n",
    "\n",
    "    print('Mean Relative Humidity:', df['y'].mean())\n",
    "\n",
    "    # Train/test split\n",
    "    train_df = df.iloc[:-horizon].copy()\n",
    "    test_df = df.iloc[-horizon:].copy()\n",
    "\n",
    "    # Add unique_id\n",
    "    train_df['unique_id'] = unique_id\n",
    "    test_df['unique_id'] = unique_id\n",
    "\n",
    "    return train_df[['unique_id', 'ds', 'y']], test_df[['unique_id', 'ds', 'y']]\n",
    "\n",
    "gptoss_prediction(load_relative_humidity, model, tokenizer, horizon=24, frequency=\"M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94416a3d-38cd-4aa1-9c39-b0e49aac2620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_birth(horizon=24, unique_id=\"AP1\"):\n",
    "    df = pd.read_csv(\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-total-female-births.csv\")\n",
    "\n",
    "    # Remove unnamed columns\n",
    "    if 'Unnamed: 2' in df.columns:\n",
    "        df = df.drop(columns=['Unnamed: 2'])\n",
    "\n",
    "    # Parse date safely\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "\n",
    "    # Drop rows with invalid dates (like the description text)\n",
    "    df = df.dropna(subset=['Date'])\n",
    "\n",
    "    # Rename columns\n",
    "    df = df.rename(columns={\"Date\": \"ds\", \"Births\": \"y\"})\n",
    "\n",
    "    # Convert 'y' to numeric, coercing errors to NaN and dropping them\n",
    "    df['y'] = pd.to_numeric(df['y'], errors='coerce')\n",
    "    df = df.dropna(subset=['y'])\n",
    "\n",
    "    print('Mean:', df['y'].mean())\n",
    "    print(df.shape)\n",
    "\n",
    "    # Train/test split\n",
    "    train_df = df.iloc[:-horizon].copy()\n",
    "    test_df = df.iloc[-horizon:].copy()\n",
    "\n",
    "    # Add unique_id\n",
    "    train_df['unique_id'] = unique_id\n",
    "    test_df['unique_id'] = unique_id\n",
    "\n",
    "    return train_df[['unique_id', 'ds', 'y']], test_df[['unique_id', 'ds', 'y']]\n",
    "\n",
    "gptoss_prediction(load_birth, model, tokenizer, horizon=24, frequency=\"M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fed63db-7259-46e8-ba3a-82d8138667ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_store(horizon=24, unique_id=\"AP1\"):\n",
    "    df = pd.read_csv(\"https://raw.githubusercontent.com/ahamed14051/time-series-dataset/refs/heads/main/store.csv\")\n",
    "    df = df[(df['store'] == 0) & (df['product'] == 0)]\n",
    "    df = df.drop(columns=['store', 'product'])\n",
    "\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "\n",
    "    # Drop rows with invalid dates (like the description text)\n",
    "    df = df.dropna(subset=['Date'])\n",
    "\n",
    "    # Rename columns\n",
    "    df = df.rename(columns={\"Date\": \"ds\", \"number_sold\": \"y\"})\n",
    "\n",
    "    # Convert 'y' to numeric, coercing errors to NaN and dropping them\n",
    "    df['y'] = pd.to_numeric(df['y'], errors='coerce')\n",
    "    df = df.dropna(subset=['y'])\n",
    "\n",
    "    print('Mean:', df['y'].mean())\n",
    "    print(df.shape)\n",
    "\n",
    "    # Train/test split\n",
    "    train_df = df.iloc[:-horizon].copy()\n",
    "    test_df = df.iloc[-horizon:].copy()\n",
    "\n",
    "    # Add unique_id\n",
    "    train_df['unique_id'] = unique_id\n",
    "    test_df['unique_id'] = unique_id\n",
    "\n",
    "    return train_df[['unique_id', 'ds', 'y']], test_df[['unique_id', 'ds', 'y']]\n",
    "\n",
    "gptoss_prediction(load_store, model, tokenizer, horizon=24, frequency=\"M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1e1707-2b8a-40eb-aa7b-ddf337781800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hospitality(horizon=24, unique_id=\"AP1\"):\n",
    "    df = pd.read_csv(\"https://raw.githubusercontent.com/ahamed14051/time-series-dataset/refs/heads/main/HospitalityEmployees.csv\")\n",
    "\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "\n",
    "    # Drop rows with invalid dates (like the description text)\n",
    "    df = df.dropna(subset=['Date'])\n",
    "\n",
    "    # Rename columns\n",
    "    df = df.rename(columns={\"Date\": \"ds\", \"Employees\": \"y\"})\n",
    "\n",
    "    # Convert 'y' to numeric, coercing errors to NaN and dropping them\n",
    "    df['y'] = pd.to_numeric(df['y'], errors='coerce')\n",
    "    df = df.dropna(subset=['y'])\n",
    "\n",
    "    print('Mean:', df['y'].mean())\n",
    "    print(df.shape)\n",
    "\n",
    "    # Train/test split\n",
    "    train_df = df.iloc[:-horizon].copy()\n",
    "    test_df = df.iloc[-horizon:].copy()\n",
    "\n",
    "    # Add unique_id\n",
    "    train_df['unique_id'] = unique_id\n",
    "    test_df['unique_id'] = unique_id\n",
    "\n",
    "    return train_df[['unique_id', 'ds', 'y']], test_df[['unique_id', 'ds', 'y']]\n",
    "\n",
    "gptoss_prediction(load_hospitality, model, tokenizer, horizon=24, frequency=\"M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccd7aad-85d3-432e-9d06-d8d53443429c",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_results = {\n",
    "\n",
    "'load_air_passengers': {'horizon': 24,\n",
    "  'MAE': 55.58,\n",
    "  'RMSE': 60.91,\n",
    "  'MAPE (%)': 12.0,\n",
    "  'R²': 0.3346},\n",
    "\n",
    " 'load_air_sunsopts': {'horizon': 120,\n",
    "  'MAE': 39.16,\n",
    "  'RMSE': 47.6,\n",
    "  'MAPE (%)': 708.45,\n",
    "  'R²': -0.277},\n",
    "\n",
    "  'load_temp': {'horizon': 240,\n",
    "  'MAE': \"\",\n",
    "  'RMSE': \"\",\n",
    "  'MAPE (%)': \"\",\n",
    "  'R²': \"\"},\n",
    "\n",
    "  'load_temperature': {'horizon': 24,\n",
    "  'MAE': 3.73,\n",
    "  'RMSE': 4.52,\n",
    "  'MAPE (%)': 28.77,\n",
    "  'R²': 0.5822},\n",
    "\n",
    " 'load_precipitation': {'horizon': 24,\n",
    "  'MAE': 266.94,\n",
    "  'RMSE': 463.79,\n",
    "  'MAPE (%)': 100.0,\n",
    "  'R²': -0.4954},\n",
    "\n",
    " 'load_humidity': {'horizon': 24,\n",
    "  'MAE': 2.55,\n",
    "  'RMSE': 2.97,\n",
    "  'MAPE (%)': 18.7,\n",
    "  'R²': 0.5559},\n",
    "\n",
    " 'load_relative_humidity': {'horizon': 24,\n",
    "  'MAE': 28.43,\n",
    "  'RMSE': 33.3,\n",
    "  'MAPE (%)': 42.09,\n",
    "  'R²': -2.3648},\n",
    "\n",
    " 'load_birth': {'horizon': 24,\n",
    "  'MAE': 5.75,\n",
    "  'RMSE': 6.46,\n",
    "  'MAPE (%)': 13.76,\n",
    "  'R²': -0.0442},\n",
    "\n",
    "  'load_store': {'horizon': 24,\n",
    "  'MAE': 15.21,\n",
    "  'RMSE': 17.78,\n",
    "  'MAPE (%)': 1.82,\n",
    "  'R²': -1.1633},\n",
    "\n",
    " 'load_hospitality': {'horizon': 24,\n",
    "  'MAE': 19.56,\n",
    "  'RMSE': 24.21,\n",
    "  'MAPE (%)': 0.99,\n",
    "  'R²': 0.5722}\n",
    "\n",
    "  }\n",
    "\n",
    "df = pd.DataFrame(GPT_results).T\n",
    "df.to_csv('GPT_results.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e9e16b-a73b-4441-ae18-2be19520bcd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
